{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as ospj\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import datetime\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from network.model import build_model\n",
    "from core.checkpoint import CheckpointIO\n",
    "# from dataset.data_loader import InputFetcher\n",
    "from dataset.frame_dataset import FramesDataset, MotionDataset, DatasetRepeater\n",
    "import network.utils as utils\n",
    "import yaml\n",
    "import random\n",
    "from utils import Bar,Logger, AverageMeter\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "import pytorch_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/train2.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "config = Munch(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device: True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device:\", use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config.seed)\n",
    "torch.cuda.manual_seed_all(config.seed)\n",
    "np.random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.args.lr = float(self.args.lr)\n",
    "        self.args.weight_decay = float(self.args.weight_decay)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.start = 0\n",
    "\n",
    "        self.nets, self.nets_ema = build_model(args)\n",
    "        # below setattrs are to make networks be children of Solver, e.g., for self.to(self.device)\n",
    "        for name, module in self.nets.items():\n",
    "            utils.print_network(module, name)\n",
    "            setattr(self, name, module)\n",
    "        for name, module in self.nets_ema.items():\n",
    "            setattr(self, name + '_ema', module)\n",
    "\n",
    "        if args.mode == 'train':\n",
    "            self.optims = Munch()\n",
    "            for net in self.nets.keys():\n",
    "                if net == 'fan':\n",
    "                    continue\n",
    "                self.optims[net] = torch.optim.Adam(\n",
    "                    params=self.nets[net].parameters(),\n",
    "                    lr=float(args.lr),\n",
    "                    betas=[args.beta1, args.beta2])\n",
    "\n",
    "            self.ckptios = [\n",
    "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets.ckpt'), **self.nets),\n",
    "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets_ema.ckpt'), **self.nets_ema),\n",
    "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_optims.ckpt'), **self.optims)]\n",
    "        else:\n",
    "            self.ckptios = [CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets_ema.ckpt'), **self.nets_ema)]\n",
    "    \n",
    "\n",
    "        self.to(self.device)\n",
    "        for name, network in self.named_children():\n",
    "            # Do not initialize the FAN parameters\n",
    "            if ('ema' not in name) and ('fan' not in name):\n",
    "                print('Initializing %s...' % name)\n",
    "                network.apply(utils.he_init)\n",
    "\n",
    "    def _save_checkpoint(self, step):\n",
    "        for ckptio in self.ckptios:\n",
    "            ckptio.save(step)\n",
    "\n",
    "    def _load_checkpoint(self, step):\n",
    "        for ckptio in self.ckptios:\n",
    "            ckptio.load(step)\n",
    "\n",
    "    def _reset_grad(self):\n",
    "        for optim in self.optims.values():\n",
    "            optim.zero_grad()\n",
    "\n",
    "    def train(self, loaders):\n",
    "        args = self.args\n",
    "        nets = self.nets\n",
    "        nets_ema = self.nets_ema\n",
    "        \n",
    "        for name in nets:\n",
    "            nets[name] = DataParallel(nets[name])\n",
    "        optims = self.optims\n",
    "\n",
    "        # fetch random validation images for debugging\n",
    "#         fetcher = InputFetcher(loaders.src, None, args.latent_dim, 'train')\n",
    "#         fetcher_val = InputFetcher(loaders.val, None, args.latent_dim, 'val')\n",
    "#         inputs_val = next(fetcher_val)\n",
    "\n",
    "        # resume training if necessary\n",
    "        if args.resume_iter > 0:\n",
    "            self._load_checkpoint(args.resume_iter)\n",
    "\n",
    "        # remember the initial value of ds weight\n",
    "        initial_lambda_ds = args.lambda_ds\n",
    "        # batch\n",
    "#         for i in range(args.resume_iter, args.total_iters):\n",
    "        for epoch in range(args.resume_iter, args.epochs):\n",
    "            bar = tqdm(total=len(loaders.src), leave=False)\n",
    "            wgan_loss, d_reg_loss, g_latent_loss, g_cycle_loss = AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter()\n",
    "            for i, inputs in enumerate(loaders.src):\n",
    "                # fetch images and labels\n",
    "    #             inputs = next(fetcher)\n",
    "                x_source, y_drive = inputs['source'].to(self.device), inputs['target']\n",
    "    #             z_trg, z_trg2 = inputs.z_trg, inputs.z_trg2\n",
    "                num_frame = len(y_drive)\n",
    "                for f in range(num_frame):\n",
    "                    # train the discriminator\n",
    "                    d_loss, d_losses_latent = compute_d_loss(nets, args, x_source, y_drive[f].to(self.device), device=self.device)\n",
    "                    self._reset_grad()\n",
    "                    d_loss.backward()\n",
    "                    optims.discriminator.step()\n",
    "\n",
    "                    # train the generator\n",
    "                    g_loss, g_losses_latent = compute_g_loss(nets, args, x_source, y_drive[f].to(self.device), device=self.device)\n",
    "                    self._reset_grad()\n",
    "                    g_loss.backward()\n",
    "                    optims.generator.step()\n",
    "\n",
    "                    moving_average(nets.generator, nets_ema.generator, beta=0.999)\n",
    "\n",
    "                    wgan_loss.update(float(d_losses_latent.wgangp), x_source.size(0))\n",
    "                    d_reg_loss.update(float(d_losses_latent.reg), x_source.size(0))\n",
    "                    g_latent_loss.update(float(g_losses_latent.adv), x_source.size(0))\n",
    "                    g_cycle_loss.update(float(g_losses_latent.cyc), x_source.size(0))\n",
    "                \n",
    "                bar.set_description(\"Epoch:{:d}, WGP: {:.6f}, R1reg: {:.2f}, G: {:.6f}, Cyc: {:.6f}\".format(\n",
    "                                    epoch+1, wgan_loss.avg, d_reg_loss.avg, g_latent_loss.avg, g_cycle_loss.avg), refresh=True)\n",
    "                bar.update()\n",
    "            bar.close()\n",
    "\n",
    "                # save model checkpoints\n",
    "            logger.append([str(wgan_loss.avg)[:8], str(d_reg_loss.avg)[:8], str(g_latent_loss.avg)[:8], str(g_cycle_loss.avg)[:8]])\n",
    "            if (epoch+1) % config.save_every == 0:\n",
    "                self._save_checkpoint(step=epoch+1)\n",
    "\n",
    "            # compute SSIM and FID in test_set\n",
    "            if (epoch+1) % config.eval_every == 0:\n",
    "                self.evaluate(epoch, nets.generator, loaders.val)\n",
    "        \n",
    "        self.evaluate(epoch, nets.generator, loaders.val)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, epoch, net, loader):\n",
    "        if not os.path.isdir(config.result_dir):\n",
    "            os.makedirs(config.result_dir)\n",
    "        result_target = os.path.join(config.result_dir, 'tar')\n",
    "        result_gen = os.path.join(config.result_dir,'gen')\n",
    "        if not os.path.isdir(result_target):\n",
    "            os.makedirs(result_target)\n",
    "        if not os.path.isdir(result_gen):\n",
    "            os.makedirs(result_gen)\n",
    "        \n",
    "        bar = tqdm(total=len(loader), leave=False)\n",
    "        ssim_meter, fid_meter = AverageMeter(), AverageMeter()\n",
    "        for iteration, x in enumerate(loader):\n",
    "            test_video = torch.tensor(np.concatenate(x['video'])) # (frame, c, w, h)\n",
    "            num_frame = test_video.shape[0]\n",
    "            k_frame = np.random.choice(num_frame-config.K, size=1, replace=False)\n",
    "            source = test_video[k_frame]\n",
    "            source_gen = net(source.to(self.device))\n",
    "            target = test_video[k_frame+config.K]\n",
    "            \n",
    "            ssim = float(pytorch_ssim.ssim(source_gen, target.to(self.device)))\n",
    "            ssim_meter.update(ssim, iteration+1)\n",
    "            \n",
    "            # save for FID\n",
    "            gen = source_gen.squeeze().cpu().detach().numpy()\n",
    "            target = target.squeeze().cpu().detach().numpy()\n",
    "            gen = gen.swapaxes(0, 1).swapaxes(1, 2)\n",
    "            target = target.swapaxes(0, 1).swapaxes(1, 2)\n",
    "            gen_img = Image.fromarray((gen*255).astype('uint8'))\n",
    "            tar_img = Image.fromarray((target*255).astype('uint8'))\n",
    "            gen_img.save(result_gen + '/{}.png'.format(iteration+1))\n",
    "            tar_img.save(result_target + '/{}.png'.format(iteration+1))\n",
    "            \n",
    "            bar.set_description(\"Epoch:{:d}, SSIM: {:.8f}\".format(epoch+1, ssim_meter.avg), refresh=True)\n",
    "            bar.update()\n",
    "        bar.close()\n",
    "        val_logger.append([str(ssim_meter.avg)])\n",
    "        return\n",
    "            \n",
    "def compute_d_loss(nets, args, x_real, y_org, z_trg=None, x_ref=None, masks=None, device='cuda'):\n",
    "#     assert (z_trg is None) != (x_ref is None)\n",
    "    # with real images\n",
    "    x_real.requires_grad = True\n",
    "    y_org.requires_grad = True\n",
    "    real_out = nets.discriminator(y_org)\n",
    "     # with fake images\n",
    "    with torch.no_grad():\n",
    "        x_fake = nets.generator(x_real)\n",
    "    fake_out = nets.discriminator(x_fake)\n",
    "    loss_reg = r1_reg(real_out, y_org)\n",
    "    \n",
    "    loss = (torch.mean(fake_out) - torch.mean(real_out) + (args.drift * torch.mean(real_out ** 2)))\n",
    "    x_fake.requires_grad = True\n",
    "    gp = gradient_penalty(nets, y_org, x_fake, args.lambda_gp, device=device)\n",
    "    loss += gp\n",
    "#     loss_fake = adv_loss(out, 0)\n",
    "\n",
    "    loss = loss + args.lambda_reg * loss_reg\n",
    "    return loss, Munch(wgangp=loss.item(), reg=loss_reg.item())\n",
    "\n",
    "\n",
    "def compute_g_loss(nets, args, x_real, y_org, z_trgs=None, x_refs=None, masks=None, device='cuda'):\n",
    "#     assert (z_trgs is None) != (x_refs is None)\n",
    "#     if z_trgs is not None:\n",
    "#         z_trg, z_trg2 = z_trgs\n",
    "\n",
    "    # adversarial loss\n",
    "    x_fake = nets.generator(x_real)\n",
    "    out = nets.discriminator(x_fake)\n",
    "    loss_adv = adv_loss(out, 1)\n",
    "\n",
    "#     # style reconstruction loss\n",
    "#     s_pred = nets.style_encoder(x_fake, y_org)\n",
    "#     loss_sty = torch.mean(torch.abs(s_pred - s_trg))\n",
    "\n",
    "    # diversity sensitive loss: same real image --> different reference (y_trg, s_trg)\n",
    "#     if z_trgs is not None:\n",
    "#         s_trg2 = nets.mapping_network(z_trg2, y_trg)\n",
    "#     else:\n",
    "#         s_trg2 = nets.style_encoder(x_ref2, y_trg)\n",
    "#     x_fake2 = nets.generator(x_real, s_trg2, masks=masks)\n",
    "#     x_fake2 = x_fake2.detach()\n",
    "#     loss_ds = torch.mean(torch.abs(x_fake - x_fake2))\n",
    "\n",
    "    # cycle-consistency loss\n",
    "#     masks = nets.fan.get_heatmap(x_fake) if args.w_hpf > 0 else None\n",
    "#     s_org = nets.style_encoder(x_real, y_org)\n",
    "    x_rec = nets.generator(x_fake)\n",
    "    loss_cyc = torch.mean(torch.abs(x_rec - x_real))\n",
    "\n",
    "#     loss = loss_adv + args.lambda_sty * loss_sty - args.lambda_ds * loss_ds + args.lambda_cyc * loss_cyc\n",
    "    loss = loss_adv + args.lambda_cyc * loss_cyc\n",
    "    return loss, Munch(adv=loss_adv.item(), cyc=loss_cyc.item())\n",
    "\n",
    "def gradient_penalty(nets, real, fake, reg_lambda=10, device='cuda'):\n",
    "    from torch.autograd import grad\n",
    "    batch_size = real.shape[0]\n",
    "    epsilon = torch.rand(batch_size, 1, 1, 1).to(device)\n",
    "    merged = (epsilon * real) + ((1 - epsilon) * fake)\n",
    "    # forward\n",
    "    op = nets.discriminator(merged)\n",
    "    \n",
    "    # merted gradient\n",
    "    gradient = grad(outputs=op, inputs=merged, create_graph=True, grad_outputs=torch.ones_like(op), \n",
    "                    retain_graph=True, only_inputs=True)[0]\n",
    "    \n",
    "    # calc penalty\n",
    "    penalty = reg_lambda * ((gradient.norm(p=2, dim=1) - 1) ** 2).mean()\n",
    "    return penalty\n",
    "    \n",
    "\n",
    "def moving_average(model, model_test, beta=0.999):\n",
    "    for param, param_test in zip(model.parameters(), model_test.parameters()):\n",
    "        param_test.data = torch.lerp(param.data, param_test.data, beta)\n",
    "\n",
    "\n",
    "def adv_loss(logits, target):\n",
    "    assert target in [1, 0]\n",
    "    targets = torch.full_like(logits, fill_value=target)\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def r1_reg(d_out, x_in):\n",
    "    # zero-centered gradient penalty for real images\n",
    "    batch_size = x_in.size(0)\n",
    "    grad_dout = torch.autograd.grad(\n",
    "        outputs=d_out.sum(), inputs=x_in,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    grad_dout2 = grad_dout.pow(2)\n",
    "    assert(grad_dout2.size() == x_in.size())\n",
    "    reg = 0.5 * grad_dout2.view(batch_size, -1).sum(1).mean(0)\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use predefined train-test split.\n",
      "Use predefined train-test split.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MotionDataset(config.root_dir, image_shape=config.frame_shape, id_sampling=True, is_train=True, random_seed=config.seed)\n",
    "test_dataset = FramesDataset(config.root_dir, image_shape=config.frame_shape, id_sampling=True, is_train=False, random_seed=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = DatasetRepeater(train_dataset, config.num_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, \n",
    "                              num_workers=config.num_workers, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = Munch(src=train_loader, val=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of generator: 33892995\n",
      "Number of parameters of discriminator: 20851777\n",
      "Initializing generator...\n",
      "Initializing discriminator...\n"
     ]
    }
   ],
   "source": [
    "solver = Solver(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    logger = Logger(os.path.join(config.checkpoint_dir, 'log.txt'), resume=True)\n",
    "    val_logger = Logger(os.path.join(config.checkpoint_dir, 'val_log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(config.checkpoint_dir, 'log.txt'))\n",
    "    val_logger = Logger(os.path.join(config.checkpoint_dir, 'val_log.txt'))\n",
    "    logger.set_names(['WGAN-GP Loss', 'R1reg Loss', 'G-latent-adv Loss', 'G-cyclc Loss'])\n",
    "    val_logger.set_names(['SSIM measure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from logs/checkpoints_motion/000165_nets.ckpt...\n",
      "Loading checkpoint from logs/checkpoints_motion/000165_nets_ema.ckpt...\n",
      "Loading checkpoint from logs/checkpoints_motion/000165_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000170_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000170_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000170_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000175_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000175_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000175_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000180_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000180_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000180_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000185_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000185_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000185_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000190_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000190_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000190_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000195_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000195_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000195_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000200_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000200_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000200_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000205_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000205_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000205_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000210_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000210_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000210_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000215_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000215_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000215_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000220_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000220_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000220_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000225_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000225_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000225_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000230_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000230_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000230_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000235_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000235_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000235_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000240_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000240_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000240_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000245_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000245_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000245_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_motion/000250_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000250_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_motion/000250_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:251, WGP: -0.827599, R1reg: 10.01, G: 41.247370, Cyc: 0.159266:  96%|█████████▌| 25/26 [08:12<00:19, 19.38s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c1f0897ef4d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-573bdad6702a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, loaders)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_losses_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_d_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_drive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0moptims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "solver.train(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
