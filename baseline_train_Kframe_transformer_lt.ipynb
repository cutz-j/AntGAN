{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as ospj\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import datetime\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import deque\n",
    "\n",
    "from network.model_zt import build_model\n",
    "from core.checkpoint import CheckpointIO\n",
    "from dataset.frame_dataset import FramesDataset, MotionDataset, DatasetRepeater\n",
    "import network.utils as utils\n",
    "import yaml\n",
    "import random\n",
    "from utils import Logger, AverageMeter, center\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "import pytorch_ssim\n",
    "from torchvision import transforms\n",
    "from network.vgg import VGG\n",
    "from torchvision.models import vgg19\n",
    "import imp\n",
    "from network.wing import FAN, HighPass\n",
    "from network.interp import AntiAliasInterpolation2d\n",
    "import copy\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/train_transformer7.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "config = Munch(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device  True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = '0,1,2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device \" , use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config.seed)\n",
    "torch.cuda.manual_seed_all(config.seed)\n",
    "np.random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePyramide(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Create image pyramide for computing pyramide perceptual loss. See Sec 3.3\n",
    "    \"\"\"\n",
    "    def __init__(self, scales, num_channels):\n",
    "        super(ImagePyramide, self).__init__()\n",
    "        downs = {}\n",
    "        for scale in scales:\n",
    "            downs[str(scale).replace('.', '-')] = AntiAliasInterpolation2d(num_channels, scale)\n",
    "        self.downs = nn.ModuleDict(downs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_dict = {}\n",
    "        for scale, down_module in self.downs.items():\n",
    "            out_dict['prediction_' + str(scale).replace('-', '.')] = down_module(x)\n",
    "        return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.args.lr = float(self.args.lr)\n",
    "        self.args.weight_decay = float(self.args.weight_decay)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.start = 10\n",
    "        self.replay_memory = 10000\n",
    "        self.replay_buffer = deque(maxlen=self.replay_memory)\n",
    "\n",
    "        self.nets = build_model(args)\n",
    "        # below setattrs are to make networks be children of Solver, e.g., for self.to(self.device)\n",
    "        for name, module in self.nets.items():\n",
    "            utils.print_network(module, name)\n",
    "            setattr(self, name, module)\n",
    "\n",
    "        if args.mode == 'train':\n",
    "            self.optims = Munch()\n",
    "            self.scheduler = []\n",
    "            for net in self.nets.keys():\n",
    "                self.optims[net] = torch.optim.Adam(params=self.nets[net].parameters(), lr=float(args.lr), betas=[args.beta1, args.beta2],\n",
    "                                                   weight_decay=0)\n",
    "                sd = MultiStepLR(self.optims[net], args.lr_decay, gamma=0.1, last_epoch=-1)\n",
    "                self.scheduler.append(sd)\n",
    "\n",
    "            self.ckptios = [\n",
    "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets.ckpt'), **self.nets),\n",
    "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_optims.ckpt'), **self.optims)]\n",
    "    \n",
    "\n",
    "        self.to(self.device)\n",
    "        for name, network in self.named_children():\n",
    "            # Do not initialize the FAN parameters\n",
    "            if ('ema' not in name) and ('fan' not in name):\n",
    "                print('Initializing %s...' % name)\n",
    "                network.apply(utils.he_init)\n",
    "        \n",
    "        # landmark\n",
    "        self.fan = FAN(fname_pretrained=config.fname_fan)\n",
    "        self.fan.to(self.device)\n",
    "        self.fan.eval()\n",
    "        self.hpf = HighPass(config.w_hpf, self.device)\n",
    "        self.masking = transforms.RandomErasing(p=1.0, scale=(0.25, 0.33), ratio=(0.3, 0.33))\n",
    "        self.clip_num = config.clip_num\n",
    "        \n",
    "        # perceptual loss\n",
    "        self.vgg = VGG()\n",
    "        MainModel = imp.load_source(\"MainModel\", args.fname_ir)\n",
    "        weight = torch.load(args.fname_vgg, map_location='cpu')\n",
    "        self.vgg.load_state_dict(weight.state_dict(), strict=False)\n",
    "        self.vgg.eval()\n",
    "        self.vgg.to(self.device)\n",
    "        self.vgg19 = vgg19(pretrained=True)\n",
    "        self.vgg19.eval()\n",
    "        self.vgg19.to(self.device)\n",
    "        \n",
    "        # multiscale\n",
    "        self.image_pyramid = ImagePyramide(args.scales, 3).to(self.device)\n",
    "        \n",
    "\n",
    "    def _save_checkpoint(self, step):\n",
    "        for ckptio in self.ckptios:\n",
    "            ckptio.save(step)\n",
    "\n",
    "    def _load_checkpoint(self, step):\n",
    "        for ckptio in self.ckptios:\n",
    "            ckptio.load(step)\n",
    "\n",
    "    def _reset_grad(self):\n",
    "        for optim in self.optims.values():\n",
    "            optim.zero_grad()\n",
    "            \n",
    "    def train(self, loaders):\n",
    "        args = self.args\n",
    "        nets = self.nets\n",
    " \n",
    "        for name in nets:\n",
    "            nets[name] = DataParallel(nets[name])\n",
    "            nets[name] = nets[name].to(self.device)\n",
    "        optims = self.optims\n",
    "\n",
    "        # resume training if necessary\n",
    "        if args.resume_iter > 0:\n",
    "            self._load_checkpoint(args.resume_iter)\n",
    "\n",
    "        # batch\n",
    "        for epoch in range(args.resume_iter, args.epochs):\n",
    "            bar = tqdm(total=len(loaders.src), leave=False)\n",
    "            wgan_loss, d_reg_loss = AverageMeter(), AverageMeter()\n",
    "            g_latent_loss, vgg_loss, fm_loss, cm_loss = AverageMeter(), AverageMeter(), AverageMeter(), AverageMeter()\n",
    "            for i, inputs in enumerate(loaders.src):\n",
    "                x_source, y_drive = inputs['source'], inputs['target']\n",
    "                x_source_land = self.fan.get_landmark(x_source.to(self.device))\n",
    "                num_frame = len(y_drive)\n",
    "                x_source_mb = x_source.unsqueeze(1).repeat(1, args.K, 1, 1, 1)\n",
    "                x_source_land = F.interpolate(self.fan.get_landmark(x_source.cuda()), size=256, mode='bilinear')\n",
    "                x_source_land_mb = x_source_land.unsqueeze(1).repeat(1, args.K, 1, 1, 1)\n",
    "                y_drive_mb = y_drive[0].unsqueeze(1)\n",
    "                y_drive_land = F.interpolate(self.fan.get_landmark(y_drive[0].cuda()), size=256, mode='bilinear')\n",
    "                y_drive_land = y_drive_land.unsqueeze(1)\n",
    "\n",
    "                for f in range(1, num_frame):\n",
    "                    y_drive_ld = self.fan.get_landmark(y_drive[f].cuda())\n",
    "                    y_drive_ld = F.interpolate(y_drive_ld, size=y_drive[f].size(2), mode='bilinear') # (bs, K, 1, w, h)\n",
    "                    y_drive_mb = torch.cat([y_drive_mb, y_drive[f].unsqueeze(1)], dim=1) # (bs, K, 3, w, h)\n",
    "                    y_drive_land = torch.cat([y_drive_land, y_drive_ld.unsqueeze(1)], dim=1) # (bs, K, 1, w,h)\n",
    "                \n",
    "#                     self.replay_buffer.append((, y_drive[f]))           \n",
    "#                     if len(self.replay_buffer) < self.start:\n",
    "#                         continue\n",
    "#                     minibatch = random.sample(self.replay_buffer, 1)\n",
    "#                     x_source_mb, y_drive_mb = minibatch[0][0], minibatch[0][1]\n",
    "                    \n",
    "                k = np.random.randint(args.K)\n",
    "                # Transformer computation\n",
    "                e_hat = compute_tf_loss(nets, args, x_source_mb, y_drive_mb, x_source_land_mb, y_drive_land, device=self.device)\n",
    "\n",
    "                # Generator\n",
    "                g_loss, g_losses_latent = compute_g_loss(nets, args, x_source, y_drive_mb[:,k,:,:,:], e_hat, \n",
    "                                                         x_source_land, y_drive_land[:,k,:,:,:], \n",
    "                                                         vgg=self.vgg, vgg19=self.vgg19,\n",
    "                                                         image_pyramid=self.image_pyramid, scales=args.scales,\n",
    "                                                         device=self.device)        \n",
    "                self._reset_grad()\n",
    "                g_loss.backward()\n",
    "                optims.generator.step()\n",
    "                optims.transformer.step()\n",
    "\n",
    "                # Discriminator\n",
    "                d_loss, d_losses_latent = compute_d_loss(nets, args, x_source, y_drive_mb[:,k,:,:], e_hat, \n",
    "                                                               x_source_land, y_drive_land[:,k,:,:,:], \n",
    "                                                               image_pyramid=self.image_pyramid, scales=args.scales,\n",
    "                                                               device=self.device) \n",
    "                self._reset_grad()\n",
    "                d_loss.backward()\n",
    "                optims.discriminator.step()\n",
    "\n",
    "                wgan_loss.update(float(d_losses_latent.wgangp), x_source.size(0))\n",
    "                d_reg_loss.update(float(d_losses_latent.reg), x_source.size(0))\n",
    "                g_latent_loss.update(float(g_losses_latent.adv), x_source.size(0))\n",
    "    #                     g_cycle_loss.update(float(g_losses_latent.cyc), x_source.size(0))\n",
    "                vgg_loss.update(float(g_losses_latent.vgg), x_source.size(0))\n",
    "                fm_loss.update(float(g_losses_latent.fm), x_source.size(0))\n",
    "                cm_loss.update(float(g_losses_latent.cm), x_source.size(0))\n",
    "\n",
    "                bar.set_description(\"Ep:{:d}, D: {:.6f}, R1: {:.2f}, G: {:.6f}, Vgg: {:.6f}, FM: {:.6f}, CM: {:.6f}\".format(\n",
    "                    epoch+1, wgan_loss.avg, d_reg_loss.avg, \n",
    "                    g_latent_loss.avg, vgg_loss.avg, fm_loss.avg, cm_loss.avg), refresh=True)\n",
    "                bar.update()\n",
    "            bar.close()\n",
    "            \n",
    "            for sc in self.scheduler:\n",
    "                sc.step()\n",
    "\n",
    "                # save model checkpoints\n",
    "            logger.append([str(wgan_loss.avg)[:8], str(d_reg_loss.avg)[:8], \n",
    "                           str(g_latent_loss.avg)[:8], str(vgg_loss.avg)[:8], str(fm_loss.avg)[:8], str(cm_loss.avg)[:8]])\n",
    "            if (epoch+1) % config.save_every == 0:\n",
    "                self._save_checkpoint(step=epoch+1)\n",
    "\n",
    "            # compute SSIM and FID in test_set\n",
    "            if (epoch+1) % config.eval_every == 0:\n",
    "                self.evaluate(args, epoch, nets, loaders.val)\n",
    "                \n",
    "            self.make_animation(args, nets, loaders)\n",
    "        \n",
    "        self.evaluate(args, epoch, nets, loaders.val)\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, args, epoch, nets, loader):\n",
    "        if not os.path.isdir(args.result_dir):\n",
    "            os.makedirs(args.result_dir)\n",
    "        result_target = os.path.join(args.result_dir, 'tar')\n",
    "        result_gen = os.path.join(args.result_dir,'gen')\n",
    "        if not os.path.isdir(result_target):\n",
    "            os.makedirs(result_target)\n",
    "        if not os.path.isdir(result_gen):\n",
    "            os.makedirs(result_gen)\n",
    "        \n",
    "        bar = tqdm(total=len(loader), leave=False)\n",
    "        ssim_meter, fid_meter = AverageMeter(), AverageMeter()\n",
    "        for iteration, x in enumerate(loader):\n",
    "            try:\n",
    "                test_video = torch.tensor(np.concatenate(x['video'])) # (frame, c, w, h)\n",
    "            except:\n",
    "                continue\n",
    "            num_frame = test_video.shape[0]\n",
    "            k_frame = np.random.choice(num_frame-args.K, size=2, replace=False)\n",
    "            source = test_video[[k_frame[0]]].cuda() # (1, 3, 256, 256)\n",
    "            target = test_video[k_frame[1]:k_frame[1]+args.K].cuda() # (8, 3, 256, 256)\n",
    "            x_source_mb = source.unsqueeze(1).repeat(1,args.K,1,1,1) # (1, 8, 3, 256, 256)\n",
    "            x_source_land = F.interpolate(self.fan.get_landmark(source.cuda()), size=256, mode='bilinear') # (1, 1, 256, 256)\n",
    "            y_drive_mb = target[[0]].unsqueeze(1) # (1, 1, 3, 256, 256) first frame\n",
    "            y_drive_land = F.interpolate(self.fan.get_landmark(target[[0]].cuda()), size=256, mode='bilinear') # (1, 1, 256, 256)\n",
    "            y_drive_land = y_drive_land.unsqueeze(1) # (1, 1, 1, 256, 256)\n",
    "            for i in range(1, args.K):\n",
    "                y_drive_ld = self.fan.get_landmark(target[[i]]) # (1, 1, 64, 64)\n",
    "                y_drive_ld = F.interpolate(y_drive_ld, size=target[[i]].size(2), mode='bilinear') # (1, 1, 256, 256)\n",
    "                y_drive_mb = torch.cat([y_drive_mb, target[[i]].unsqueeze(1)], dim=1) # (1, K, 3, 256, 256)\n",
    "                y_drive_land = torch.cat([y_drive_land, y_drive_ld.unsqueeze(1)], dim=1) # (1, K, 1, 256, 256)\n",
    "\n",
    "            x_source_land_mb = x_source_land.unsqueeze(1).repeat(1, args.K, 1, 1, 1) # (1, K, 1, 256, 256)\n",
    "            x_source_land_mb = x_source_land_mb.view(-1, 1, args.img_size, args.img_size) # (k, 1, 256, 256)\n",
    "            y_drive_land = y_drive_land.view(-1, 1, args.img_size, args.img_size) # (k, 1, 256, 256)\n",
    "            \n",
    "            out = nets.transformer(x_source_land_mb, y_drive_land)\n",
    "            out = out.view(-1, args.K, args.max_conv_dim, 1, 1) # (1, K, 512, 1, 1)\n",
    "            e_hat = out.mean(dim=1) # (bs, 512, 1, 1)\n",
    "            idx = np.random.randint(0, args.K) # choose random frame\n",
    "            source_gen = nets.generator(source, x_source_land, y_drive_mb[:,idx,:,:,:], y_drive_land[idx,:,:,:], e_hat)\n",
    "            ssim = float(pytorch_ssim.ssim(source_gen, y_drive_mb[:,idx,:,:,:]))\n",
    "            ssim_meter.update(ssim, iteration+1)\n",
    "            \n",
    "            # save for FID\n",
    "            gen = source_gen.squeeze().cpu().detach().numpy()\n",
    "            target = y_drive_mb[:,idx,:,:,:].squeeze().cpu().detach().numpy()\n",
    "            gen = gen.swapaxes(0, 1).swapaxes(1, 2)\n",
    "            target = target.swapaxes(0, 1).swapaxes(1, 2)\n",
    "            gen_img = Image.fromarray((gen*255).astype('uint8'))\n",
    "            tar_img = Image.fromarray((target*255).astype('uint8'))\n",
    "            gen_img.save(result_gen + '/{}.png'.format(iteration+1))\n",
    "            tar_img.save(result_target + '/{}.png'.format(iteration+1))\n",
    "            \n",
    "            bar.set_description(\"Epoch:{:d}, SSIM: {:.8f}\".format(epoch+1, ssim_meter.avg), refresh=True)\n",
    "            bar.update()\n",
    "        bar.close()\n",
    "        val_logger.append([str(ssim_meter.avg)])\n",
    "        return\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def make_animation(self, args, nets, loaders):\n",
    "        if not os.path.isdir(args.sample_dir):\n",
    "            os.makedirs(args.sample_dir)\n",
    "        K = 100\n",
    "        random_list = np.random.choice(len(loaders.val.dataset), replace=False, size=2)\n",
    "        source_image_idx = int(random_list[0])\n",
    "        test_video_idx = int(random_list[1])\n",
    "        train_video_idx = int(np.random.choice(len(loaders.src.dataset), size=1))\n",
    "        # test animation\n",
    "        source_image = loaders.val.dataset[source_image_idx]['video'][0]\n",
    "        source_image = source_image.unsqueeze(0).cuda() # (1, 3, 256, 256)\n",
    "        test_video = loaders.val.dataset[test_video_idx]['video'] # list [video](3, 256, 256)\n",
    "        test_frame = len(test_video) if len(test_video) < K else K\n",
    "        predict_test, predict_train = [], []\n",
    "        \n",
    "        x_source_mb = source_image.unsqueeze(1).repeat(1,K,1,1,1) # (1, K, 3, 256, 256)\n",
    "        x_source_land = F.interpolate(self.fan.get_landmark(source_image), size=256, mode='bilinear') # (1, 1, 256, 256)\n",
    "        y_drive_mb = test_video[0].unsqueeze(0) # (1, 3, 256, 256)\n",
    "        y_drive_land = F.interpolate(self.fan.get_landmark(y_drive_mb.cuda()), size=256, mode='bilinear') # (1, 1, 256, 256)\n",
    "        y_drive_land = y_drive_land.unsqueeze(1) # (1, 1, 1, 256, 256)\n",
    "        y_drive_mb = y_drive_mb.unsqueeze(1) # (1, 1, 3, 256, 256)\n",
    "        for i in range(1, test_frame):\n",
    "            y_drive_ld = self.fan.get_landmark(test_video[i].cuda().unsqueeze(0)) # (1, 1, 64, 64)\n",
    "            y_drive_ld = F.interpolate(y_drive_ld, size=test_video[i].size(2), mode='bilinear') # (1, 1, 256, 256)\n",
    "            y_drive_land = torch.cat([y_drive_land, y_drive_ld.unsqueeze(1)], dim=1) # (1, K, 1, 256, 256)\n",
    "            y_drive_mb = torch.cat([y_drive_mb, test_video[i].unsqueeze(0).unsqueeze(1)], dim=1) # (1, K, 3, 256, 256)\n",
    "            \n",
    "        y_drive_mb = y_drive_mb.cuda()\n",
    "        x_source_land_mb = x_source_land.unsqueeze(1).repeat(1, test_frame, 1, 1, 1) # (1, K, 1, 256, 256)\n",
    "        x_source_land_mb = x_source_land_mb.view(-1, 1, args.img_size, args.img_size) # (bs*k, 1, 256, 256)\n",
    "        y_drive_land = y_drive_land.view(-1, 1, args.img_size, args.img_size) # (bs*k, 1, 256, 256)\n",
    "\n",
    "        out = nets.transformer(x_source_land_mb, y_drive_land)\n",
    "        out = out.view(-1, test_frame, args.max_conv_dim, 1, 1)\n",
    "        e_hat = out.mean(dim=1) # (bs, 512, 1, 1)\n",
    "        for i in range(test_frame):\n",
    "            x_fake = nets.generator(source_image, x_source_land, y_drive_mb[:,i,:,:,:], y_drive_land[i,:,:,:], e_hat)\n",
    "            predict_test.append(x_fake.cpu().detach().numpy().squeeze().swapaxes(0, 1).swapaxes(1,2))\n",
    "        source_image = (source_image*255).cpu().squeeze().numpy().swapaxes(0,1).swapaxes(1,2).astype('uint8')\n",
    "        source_img = Image.fromarray(source_image)\n",
    "        source_img.save(config.sample_dir+ '/source.png')\n",
    "        imageio.mimsave(os.path.join(config.sample_dir, 'test_gen.mp4'), [(frame*255).astype('uint8') for frame in predict_test], fps=24)\n",
    "        imageio.mimsave(os.path.join(config.sample_dir, 'test_raw.mp4'), [(frame*255).numpy().astype('uint8').swapaxes(0,1).swapaxes(1,2) for frame in test_video], fps=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_loss(nets, args, x_real, y_org, x_land, y_land, device='cuda'):\n",
    "    x_real, y_org = x_real.to(device), y_org.to(device)\n",
    "    x_land = x_land.view(-1, x_land.size(2), x_land.size(3), x_land.size(4))\n",
    "    y_land = y_land.view(-1, y_land.size(2), y_land.size(3), y_land.size(4))\n",
    "    \n",
    "    out = nets.transformer(x_land, y_land) # (bs*K, 1, w, h) / (bs*K, 1, w, h)\n",
    "    out = out.view(-1, args.K, args.max_conv_dim, 1, 1) # (bs, K, 512, 1, 1)\n",
    "    e_hat = out.mean(dim=1) # (bs, 512, 1, 1)\n",
    "    return e_hat\n",
    "\n",
    "def compute_d_loss(nets, args, x_real, y_org, e_hat, x_landmark,  y_landmark, image_pyramid, scales, device='cuda'):\n",
    "    # with real images\n",
    "    x_real, y_org = x_real.to(device), y_org.to(device)\n",
    "    x_real.requires_grad = True\n",
    "    imp_real = image_pyramid(x_real)\n",
    "    disc_real = nets.discriminator(imp_real)\n",
    "    \n",
    "    # R1-reg\n",
    "    loss_reg = 0\n",
    "    for scale in scales:\n",
    "        key = 'output_%s' % scale\n",
    "        real_key = 'prediction_%s' % scale\n",
    "        value = r1_reg(disc_real[key], imp_real[real_key])\n",
    "        loss_reg += value.mean()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x_gen = nets.generator(x_real, x_landmark, y_org, y_landmark, e_hat)\n",
    "        img_gen = image_pyramid(x_gen)\n",
    "    \n",
    "    disc_gen = nets.discriminator(img_gen)\n",
    "    loss_d = 0\n",
    "    for scale in scales:\n",
    "        key = 'output_%s' % scale\n",
    "        value = (1 - disc_real[key])**2 + disc_gen[key]**2\n",
    "        loss_d += value.mean()\n",
    "    \n",
    "    loss = loss_d + args.lambda_reg * loss_reg\n",
    "    return loss, Munch(wgangp=loss_d.item(), reg=loss_reg.item())\n",
    "\n",
    "def compute_g_loss(nets, args, x_real, y_org, e_hat, x_landmark, y_landmark, vgg, vgg19, image_pyramid, scales, device='cuda'):\n",
    "    # adversarial loss\n",
    "    # (bs, 3, w, h) / (bs, 3, w, h) / (bs, 1, w, h) / (bs, 1, w, h)\n",
    "    x_real, y_org = x_real.to(device), y_org.to(device)\n",
    "    x_gen = nets.generator(x_real, x_landmark, y_org, y_landmark, e_hat)\n",
    "    x_gen = x_gen.to(device)\n",
    "    # image pyramid for D\n",
    "    imp_gen = image_pyramid(x_gen)\n",
    "    imp_real = image_pyramid(x_real)\n",
    "    disc_gen = nets.discriminator(imp_gen) # 'output_scale' / 'map_scale'\n",
    "    with torch.no_grad():\n",
    "        disc_real = nets.discriminator(imp_real) # 'output_scale' / 'map_scale'\n",
    "    \n",
    "    # adv loss: LSGAN\n",
    "    loss_adv = 0\n",
    "    for scale in scales:\n",
    "        key = 'output_%s' % scale\n",
    "        value = ((1 - disc_gen[key])**2).mean()\n",
    "        loss_adv += value\n",
    "        \n",
    "    # feature-matching loss\n",
    "    l1_loss = nn.L1Loss()\n",
    "    loss_fm = 0\n",
    "    for scale in scales:\n",
    "        key = 'map_%s' % scale\n",
    "        for i, (a, b) in enumerate(zip(disc_real[key], disc_gen[key])):\n",
    "            value = l1_loss(a, b)\n",
    "            loss_fm += value\n",
    "    \n",
    "    # embedding-matching loss\n",
    "    key = 'map_%s' % scales[0]\n",
    "    loss_cm = l1_loss(e_hat.squeeze(), disc_real[key].squeeze())\n",
    "    \n",
    "    # perceptual loss: vggface\n",
    "    with torch.no_grad():\n",
    "        vgg_x = vgg(y_org)\n",
    "    with torch.autograd.enable_grad():\n",
    "        vgg_xhat = vgg(x_gen)\n",
    "        \n",
    "    loss_vggface = 0\n",
    "    for x_feat, xhat_feat in zip(vgg_x, vgg_xhat):\n",
    "        loss_vggface += l1_loss(x_feat, xhat_feat)\n",
    "        \n",
    "    conv_idx_list = [2,7,12,21.30] # indexs of conv layers\n",
    "    def vgg_x_hook(module, input, output):\n",
    "            output.detach_() #no gradient compute\n",
    "            vgg_x_features.append(output)\n",
    "    def vgg_xhat_hook(module, input, output):\n",
    "        vgg_xhat_features.append(output)\n",
    "            \n",
    "    vgg_x_features = []\n",
    "    vgg_xhat_features = []\n",
    "    vgg_x_handles = []\n",
    "    conv_idx_iter = 0\n",
    "        \n",
    "    for i, m in enumerate(vgg19.features.modules()):\n",
    "        if i == conv_idx_list[conv_idx_iter]:\n",
    "            if conv_idx_iter < len(conv_idx_list)-1:\n",
    "                conv_idx_iter += 1\n",
    "            vgg_x_handles.append(m.register_forward_hook(vgg_x_hook))\n",
    "    with torch.no_grad():\n",
    "        vgg19(y_org)\n",
    "    for h in vgg_x_handles:\n",
    "        h.remove()\n",
    "    \n",
    "    vgg_xhat_handles = []\n",
    "    conv_idx_iter = 0\n",
    "    with torch.autograd.enable_grad():\n",
    "        for i, m in enumerate(vgg19.features.modules()):\n",
    "            if i == conv_idx_list[conv_idx_iter]:\n",
    "                if conv_idx_iter < len(conv_idx_list)-1:\n",
    "                    conv_idx_iter += 1\n",
    "                vgg_xhat_handles.append(m.register_forward_hook(vgg_xhat_hook))\n",
    "        vgg19(x_gen)\n",
    "        \n",
    "        for h in vgg_xhat_handles:\n",
    "            h.remove()\n",
    "    \n",
    "    loss_vgg19 = 0\n",
    "    for x_feat, xhat_feat in zip(vgg_x_features, vgg_xhat_features):\n",
    "        loss_vgg19 += l1_loss(x_feat, xhat_feat)\n",
    "    \n",
    "    loss = loss_adv + args.lambda_vggface * loss_vggface + args.lambda_vgg19 * loss_vgg19 + args.lambda_fm * loss_fm + args.lambda_fm * loss_cm\n",
    "    return loss, Munch(adv=loss_adv.item(), vgg=loss_vggface.item()+loss_vgg19.item(), fm=loss_fm.item(), cm=loss_cm.item())\n",
    "    \n",
    "def moving_average(model, model_test, beta=0.999):\n",
    "    for param, param_test in zip(model.parameters(), model_test.parameters()):\n",
    "        param_test.data = torch.lerp(param.data, param_test.data, beta)\n",
    "\n",
    "def adv_loss(logits, target):\n",
    "    assert target in [1, 0]\n",
    "    targets = torch.full_like(logits, fill_value=target)\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "    return loss\n",
    "\n",
    "def r1_reg(d_out, x_in):\n",
    "    # zero-centered gradient penalty for real images\n",
    "    batch_size = x_in.size(0)\n",
    "    grad_dout = torch.autograd.grad(\n",
    "        outputs=d_out.sum(), inputs=x_in,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    grad_dout2 = grad_dout.pow(2)\n",
    "    assert(grad_dout2.size() == x_in.size())\n",
    "    reg = 0.5 * grad_dout2.view(batch_size, -1).sum(1).mean(0)\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use predefined train-test split.\n",
      "Use predefined train-test split.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MotionDataset(config.root_dir, image_shape=config.frame_shape, id_sampling=True, is_train=True, random_seed=config.seed)\n",
    "test_dataset = FramesDataset(config.root_dir, image_shape=config.frame_shape, id_sampling=True, is_train=False, random_seed=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetRepeater(train_dataset, config.num_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, \n",
    "                              num_workers=config.num_workers, pin_memory=False, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=config.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = Munch(src=train_loader, val=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of generator: 33541923\n",
      "Number of parameters of discriminator: 29173571\n",
      "Number of parameters of transformer: 10586755\n",
      "Initializing generator...\n",
      "Initializing discriminator...\n",
      "Initializing transformer...\n"
     ]
    }
   ],
   "source": [
    "solver = Solver(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    logger = Logger(os.path.join(config.checkpoint_dir, 'log.txt'), resume=True)\n",
    "    val_logger = Logger(os.path.join(config.checkpoint_dir, 'val_log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(config.checkpoint_dir, 'log.txt'))\n",
    "    val_logger = Logger(os.path.join(config.checkpoint_dir, 'val_log.txt'))\n",
    "    logger.set_names(['D Loss', 'R1reg Loss', 'G-latent-adv Loss', 'Perceptual Loss', 'Feature-matching Loss', 'Content-matching Loss'])\n",
    "    val_logger.set_names(['SSIM measure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000001_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000001_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000002_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000002_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000003_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000003_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000004_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000004_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000005_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000005_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000006_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000006_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000007_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000007_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000008_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000008_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000009_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000009_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000010_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000010_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000011_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000011_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000012_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000012_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000013_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000013_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000014_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000014_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000015_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000015_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000016_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000016_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000017_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000017_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000018_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000018_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000019_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000019_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000020_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000020_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000021_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000021_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000022_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000022_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000023_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000023_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000024_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000024_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000025_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000025_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000026_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000026_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000027_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000027_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000028_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000028_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000029_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000029_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000030_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000030_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000031_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000031_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000032_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000032_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000033_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000033_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000034_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000034_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000035_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000035_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000036_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000036_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000037_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000037_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000038_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000038_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000039_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000039_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000040_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000040_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000041_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000041_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000042_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000042_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000043_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000043_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000044_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000044_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000045_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000045_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000046_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000046_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000047_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000047_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/tf7/checkpoints/000048_nets.ckpt...\n",
      "Saving checkpoint into logs/tf7/checkpoints/000048_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:49, D: 0.113630, R1: 0.05, G: 2.845651, Vgg: 3.298123, FM: 4.534843, CM: 0.018213:  61%|██████    | 478/783 [1:52:04<1:12:13, 14.21s/it]"
     ]
    }
   ],
   "source": [
    "solver.train(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
