{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as ospj\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import datetime\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from network.model_e import build_model\n",
    "from core.checkpoint import CheckpointIO\n",
    "# from dataset.data_loader import InputFetcher\n",
    "from dataset.frame_dataset import FramesDataset, MotionDataset, DatasetRepeater\n",
    "import network.utils as utils\n",
    "import yaml\n",
    "import random\n",
    "from utils import Bar,Logger, AverageMeter\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parallel.data_parallel import DataParallel\n",
    "import pytorch_ssim\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config/train_transformer.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "config = Munch(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device  True\n"
     ]
    }
   ],
   "source": [
    "# GPU Device\n",
    "gpu_id = '1'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device \" , use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config.seed)\n",
    "torch.cuda.manual_seed_all(config.seed)\n",
    "np.random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.args.lr = float(self.args.lr)\n",
    "        self.args.weight_decay = float(self.args.weight_decay)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.start = 0\n",
    "\n",
    "        self.nets, self.nets_ema = build_model(args)\n",
    "        # below setattrs are to make networks be children of Solver, e.g., for self.to(self.device)\n",
    "        for name, module in self.nets.items():\n",
    "            utils.print_network(module, name)\n",
    "            setattr(self, name, module)\n",
    "        for name, module in self.nets_ema.items():\n",
    "            setattr(self, name + '_ema', module)\n",
    "\n",
    "        if args.mode == 'train':\n",
    "            self.optims = Munch()\n",
    "            for net in self.nets.keys():\n",
    "                if net == 'fan':\n",
    "                    continue\n",
    "                self.optims[net] = torch.optim.Adam(\n",
    "                    params=self.nets[net].parameters(),\n",
    "                    lr=float(args.lr),\n",
    "                    betas=[args.beta1, args.beta2])\n",
    "\n",
    "            self.ckptios = [\n",
    "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets.ckpt'), **self.nets),\n",
    "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets_ema.ckpt'), **self.nets_ema),\n",
    "                CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_optims.ckpt'), **self.optims)]\n",
    "        else:\n",
    "            self.ckptios = [CheckpointIO(ospj(args.checkpoint_dir, '{:06d}_nets_ema.ckpt'), **self.nets_ema)]\n",
    "    \n",
    "\n",
    "        self.to(self.device)\n",
    "        for name, network in self.named_children():\n",
    "            # Do not initialize the FAN parameters\n",
    "            if ('ema' not in name) and ('fan' not in name):\n",
    "                print('Initializing %s...' % name)\n",
    "                network.apply(utils.he_init)\n",
    "\n",
    "    def _save_checkpoint(self, step):\n",
    "        for ckptio in self.ckptios:\n",
    "            ckptio.save(step)\n",
    "\n",
    "    def _load_checkpoint(self, step):\n",
    "        for ckptio in self.ckptios:\n",
    "            ckptio.load(step)\n",
    "\n",
    "    def _reset_grad(self):\n",
    "        for optim in self.optims.values():\n",
    "            optim.zero_grad()\n",
    "\n",
    "    def train(self, loaders):\n",
    "        args = self.args\n",
    "        nets = self.nets\n",
    "        nets_ema = self.nets_ema\n",
    "        \n",
    "        for name in nets:\n",
    "            nets[name] = DataParallel(nets[name])\n",
    "        optims = self.optims\n",
    "\n",
    "        # resume training if necessary\n",
    "        if args.resume_iter > 0:\n",
    "            self._load_checkpoint(args.resume_iter)\n",
    "\n",
    "\n",
    "        # batch\n",
    "#         for i in range(args.resume_iter, args.total_iters):\n",
    "        for epoch in range(args.resume_iter, args.epochs):\n",
    "            bar = tqdm(total=len(loaders.src), leave=False)\n",
    "            wgan_loss, d_reg_loss = AverageMeter(), AverageMeter()\n",
    "            g_latent_loss, g_cycle_loss, tf_l1_loss = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "            for i, inputs in enumerate(loaders.src):\n",
    "                x_source, y_drive = inputs['source'].to(self.device), inputs['target']\n",
    "                num_frame = len(y_drive)\n",
    "                for f in range(num_frame):\n",
    "                    # train the discriminator\n",
    "                    d_loss, d_losses_latent = compute_d_loss(nets, args, x_source, y_drive[f].to(self.device), device=self.device)\n",
    "                    self._reset_grad()\n",
    "                    d_loss.backward()\n",
    "                    optims.discriminator.step()\n",
    "\n",
    "                    # train the generator\n",
    "                    g_loss, g_losses_latent = compute_g_loss(nets, args, x_source, y_drive[f].to(self.device), device=self.device)\n",
    "                    self._reset_grad()\n",
    "                    g_loss.backward()\n",
    "                    optims.generator.step()\n",
    "                    optims.transformer.step()\n",
    "\n",
    "                    moving_average(nets.generator, nets_ema.generator, beta=0.999)\n",
    "\n",
    "                    wgan_loss.update(float(d_losses_latent.wgangp), x_source.size(0))\n",
    "                    d_reg_loss.update(float(d_losses_latent.reg), x_source.size(0))\n",
    "                    g_latent_loss.update(float(g_losses_latent.adv), x_source.size(0))\n",
    "                    g_cycle_loss.update(float(g_losses_latent.cyc), x_source.size(0))\n",
    "                    tf_l1_loss.update(float(g_losses_latent.tf), x_source.size(0))\n",
    "                    \n",
    "                \n",
    "                bar.set_description(\"Ep:{:d}, WGP: {:.6f}, R1: {:.2f}, G: {:.6f}, Cyc: {:.6f} Tf: {:.6f}\".format(\n",
    "                                    epoch+1, wgan_loss.avg, d_reg_loss.avg, \n",
    "                                    g_latent_loss.avg, g_cycle_loss.avg, tf_l1_loss.avg), refresh=True)\n",
    "                bar.update()\n",
    "            bar.close()\n",
    "\n",
    "                # save model checkpoints\n",
    "            logger.append([str(wgan_loss.avg)[:8], str(d_reg_loss.avg)[:8], \n",
    "                           str(g_latent_loss.avg)[:8], str(g_cycle_loss.avg)[:8], str(tf_l1_loss.avg)[:8]])\n",
    "            if (epoch+1) % config.save_every == 0:\n",
    "                self._save_checkpoint(step=epoch+1)\n",
    "\n",
    "            # compute SSIM and FID in test_set\n",
    "            if (epoch+1) % config.eval_every == 0:\n",
    "                self.evaluate(epoch, nets.generator, loaders.val)\n",
    "        \n",
    "        self.evaluate(epoch, nets.generator, loaders.val)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, epoch, net, loader):\n",
    "        if not os.path.isdir(config.result_dir):\n",
    "            os.makedirs(config.result_dir)\n",
    "        result_target = os.path.join(config.result_dir, 'tar')\n",
    "        result_gen = os.path.join(config.result_dir,'gen')\n",
    "        if not os.path.isdir(result_target):\n",
    "            os.makedirs(result_target)\n",
    "        if not os.path.isdir(result_gen):\n",
    "            os.makedirs(result_gen)\n",
    "        \n",
    "        bar = tqdm(total=len(loader), leave=False)\n",
    "        ssim_meter, fid_meter = AverageMeter(), AverageMeter()\n",
    "        for iteration, x in enumerate(loader):\n",
    "            test_video = torch.tensor(np.concatenate(x['video'])) # (frame, c, w, h)\n",
    "            num_frame = test_video.shape[0]\n",
    "            k_frame = np.random.choice(num_frame-config.K, size=2, replace=False)\n",
    "            source = test_video[[k_frame[0]]]\n",
    "            target = test_video[[k_frame[1]]]\n",
    "            source_gen, _, _ = net(source.to(self.device), target.to(self.device))\n",
    "            \n",
    "            ssim = float(pytorch_ssim.ssim(source_gen, target.to(self.device)))\n",
    "            ssim_meter.update(ssim, iteration+1)\n",
    "            \n",
    "            # save for FID\n",
    "            gen = source_gen.squeeze().cpu().detach().numpy()\n",
    "            target = target.squeeze().cpu().detach().numpy()\n",
    "            gen = gen.swapaxes(0, 1).swapaxes(1, 2)\n",
    "            target = target.swapaxes(0, 1).swapaxes(1, 2)\n",
    "            gen_img = Image.fromarray((gen*255).astype('uint8'))\n",
    "            tar_img = Image.fromarray((target*255).astype('uint8'))\n",
    "            gen_img.save(result_gen + '/{}.png'.format(iteration+1))\n",
    "            tar_img.save(result_target + '/{}.png'.format(iteration+1))\n",
    "            \n",
    "            bar.set_description(\"Epoch:{:d}, SSIM: {:.8f}\".format(epoch+1, ssim_meter.avg), refresh=True)\n",
    "            bar.update()\n",
    "        bar.close()\n",
    "        val_logger.append([str(ssim_meter.avg)])\n",
    "        return\n",
    "    \n",
    "    def make_animation(self, net, loader):\n",
    "        if not os.path.isdir(config.result_dir):\n",
    "            os.makedirs(config.result_dir)\n",
    "        pass\n",
    "            \n",
    "def compute_d_loss(nets, args, x_real, y_org, device='cuda'):\n",
    "#     assert (z_trg is None) != (x_ref is None)\n",
    "    # with real images\n",
    "    x_real.requires_grad = True\n",
    "    y_org.requires_grad = True\n",
    "    real_out = nets.discriminator(y_org)\n",
    "     # with fake images\n",
    "    with torch.no_grad():\n",
    "        x_fake, _, _ = nets.generator(x_real, y_org)\n",
    "    fake_out = nets.discriminator(x_fake)\n",
    "    loss_reg = r1_reg(real_out, y_org)\n",
    "    \n",
    "    loss = (torch.mean(fake_out) - torch.mean(real_out) + (args.drift * torch.mean(real_out ** 2)))\n",
    "    x_fake.requires_grad = True\n",
    "    gp = gradient_penalty(nets, y_org, x_fake, args.lambda_gp, device=device)\n",
    "    loss += gp\n",
    "\n",
    "    loss = loss + args.lambda_reg * loss_reg\n",
    "    return loss, Munch(wgangp=loss.item(), reg=loss_reg.item())\n",
    "\n",
    "\n",
    "def compute_g_loss(nets, args, x_real, y_org, device='cuda'):\n",
    "    # adversarial loss\n",
    "    x_fake, x_att, t_att = nets.generator(x_real, y_org)\n",
    "    out = nets.discriminator(x_fake)\n",
    "    loss_adv = adv_loss(out, 1)\n",
    "\n",
    "    # transformer reconstruction loss\n",
    "    tf_loss = compute_tf_loss(args, x_att, t_att)\n",
    "    \n",
    "    # diversity sensitive loss: same real image --> different reference (y_trg, s_trg)\n",
    "#     if z_trgs is not None:\n",
    "#         s_trg2 = nets.mapping_network(z_trg2, y_trg)\n",
    "#     else:\n",
    "#         s_trg2 = nets.style_encoder(x_ref2, y_trg)\n",
    "#     x_fake2 = nets.generator(x_real, s_trg2, masks=masks)\n",
    "#     x_fake2 = x_fake2.detach()\n",
    "#     loss_ds = torch.mean(torch.abs(x_fake - x_fake2))\n",
    "\n",
    "    # cycle-consistency loss\n",
    "    x_rec, x_tf, t_tf = nets.generator(x_fake, x_real)\n",
    "    loss_cyc = torch.mean(torch.abs(x_rec - x_real)) + torch.mean(torch.abs(x_tf - x_att))\n",
    "\n",
    "#     loss = loss_adv + args.lambda_sty * loss_sty - args.lambda_ds * loss_ds + args.lambda_cyc * loss_cyc\n",
    "    loss = loss_adv + args.lambda_cyc * loss_cyc + args.lambda_tf * tf_loss\n",
    "    return loss, Munch(adv=loss_adv.item(), cyc=loss_cyc.item(), tf=tf_loss.item())\n",
    "\n",
    "def compute_tf_loss(args, x_att, t_att, device='cuda'):\n",
    "    loss_tf = torch.mean(torch.abs(x_att - t_att))\n",
    "    return loss_tf\n",
    "\n",
    "def gradient_penalty(nets, real, fake, reg_lambda=10, device='cuda'):\n",
    "    from torch.autograd import grad\n",
    "    batch_size = real.shape[0]\n",
    "    epsilon = torch.rand(batch_size, 1, 1, 1).to(device)\n",
    "    merged = (epsilon * real) + ((1 - epsilon) * fake)\n",
    "    # forward\n",
    "    op = nets.discriminator(merged)\n",
    "    \n",
    "    # merted gradient\n",
    "    gradient = grad(outputs=op, inputs=merged, create_graph=True, grad_outputs=torch.ones_like(op), \n",
    "                    retain_graph=True, only_inputs=True)[0]\n",
    "    \n",
    "    # calc penalty\n",
    "    penalty = reg_lambda * ((gradient.norm(p=2, dim=1) - 1) ** 2).mean()\n",
    "    return penalty\n",
    "    \n",
    "\n",
    "def moving_average(model, model_test, beta=0.999):\n",
    "    for param, param_test in zip(model.parameters(), model_test.parameters()):\n",
    "        param_test.data = torch.lerp(param.data, param_test.data, beta)\n",
    "\n",
    "\n",
    "def adv_loss(logits, target):\n",
    "    assert target in [1, 0]\n",
    "    targets = torch.full_like(logits, fill_value=target)\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def r1_reg(d_out, x_in):\n",
    "    # zero-centered gradient penalty for real images\n",
    "    batch_size = x_in.size(0)\n",
    "    grad_dout = torch.autograd.grad(\n",
    "        outputs=d_out.sum(), inputs=x_in,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    grad_dout2 = grad_dout.pow(2)\n",
    "    assert(grad_dout2.size() == x_in.size())\n",
    "    reg = 0.5 * grad_dout2.view(batch_size, -1).sum(1).mean(0)\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use predefined train-test split.\n",
      "Use predefined train-test split.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MotionDataset(config.root_dir, image_shape=config.frame_shape, id_sampling=True, is_train=True, random_seed=config.seed)\n",
    "test_dataset = FramesDataset(config.root_dir, image_shape=config.frame_shape, id_sampling=True, is_train=False, random_seed=config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = DatasetRepeater(train_dataset, config.num_repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, \n",
    "                              num_workers=config.num_workers, pin_memory=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = Munch(src=train_loader, val=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters of generator: 56767119\n",
      "Number of parameters of discriminator: 20851777\n",
      "Number of parameters of transformer: 18914304\n",
      "Initializing generator...\n",
      "Initializing discriminator...\n",
      "Initializing transformer...\n"
     ]
    }
   ],
   "source": [
    "solver = Solver(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "# Resume\n",
    "if resume:\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    logger = Logger(os.path.join(config.checkpoint_dir, 'log.txt'), resume=True)\n",
    "    val_logger = Logger(os.path.join(config.checkpoint_dir, 'val_log.txt'), resume=True)\n",
    "else:\n",
    "    logger = Logger(os.path.join(config.checkpoint_dir, 'log.txt'))\n",
    "    val_logger = Logger(os.path.join(config.checkpoint_dir, 'val_log.txt'))\n",
    "    logger.set_names(['WGAN-GP Loss', 'R1reg Loss', 'G-latent-adv Loss', 'G-cyclc Loss', 'Transformer Loss'])\n",
    "    val_logger.set_names(['SSIM measure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from logs/checkpoints_tf/000130_nets.ckpt...\n",
      "Loading checkpoint from logs/checkpoints_tf/000130_nets_ema.ckpt...\n",
      "Loading checkpoint from logs/checkpoints_tf/000130_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_tf/000135_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_tf/000135_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_tf/000135_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_tf/000140_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_tf/000140_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_tf/000140_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint into logs/checkpoints_tf/000145_nets.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_tf/000145_nets_ema.ckpt...\n",
      "Saving checkpoint into logs/checkpoints_tf/000145_optims.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:150, WGP: 6.930871, R1: 3.09, G: 15.216420, Cyc: 0.154673 Tf: 0.000038:  85%|████████▍ | 22/26 [08:51<01:35, 23.82s/it]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c1f0897ef4d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-fe8da48f99c8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, loaders)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0;31m# train the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0md_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_losses_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_d_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_drive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fe8da48f99c8>\u001b[0m in \u001b[0;36mcompute_d_loss\u001b[0;34m(nets, args, x_real, y_org, device)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrift\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_out\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mx_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mgp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_org\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_gp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-fe8da48f99c8>\u001b[0m in \u001b[0;36mgradient_penalty\u001b[0;34m(nets, real, fake, reg_lambda, device)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0mmerged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "solver.train(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
